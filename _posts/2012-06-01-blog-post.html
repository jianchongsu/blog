--- 
layout: post
name: blog-post
title: "\xE7\xBA\xA2\xE6\xA5\xBC\xE6\xA2\xA6\xE6\x96\x87\xE6\x9C\xAC\xE6\x8A\x98\xE8\x85\xBE\xE7\xBA\xAA\xE8\xA6\x81"
date: 2012-06-01 11:11:00 +08:00
categories: 
- ggplot2
- "\xE5\xA4\x9A\xE7\xBB\xB4\xE6\xA0\x87\xE5\xBA\xA6\xE5\x88\x86\xE6\x9E\x90"
- "\xE6\x96\x87\xE6\x9C\xAC\xE6\x8C\x96\xE6\x8E\x98"
permalink: /2012/06/blog-post.html
---
<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/--Tq6KJyDMGw/T8guDBuhDeI/AAAAAAAAA7o/CO8zj-6NGqs/s1600/1.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" height="176" src="http://4.bp.blogspot.com/--Tq6KJyDMGw/T8guDBuhDeI/AAAAAAAAA7o/CO8zj-6NGqs/s200/1.jpg" width="200" /></a></div>虽然对红学一无所知，对文本挖掘只是“略懂”，但并不妨碍我们勇于折腾的生活方式。上周参加了第五届R会议，学到不少好东西，其中一个就是中文分词的工具。当然要找个对象来折腾一下啦。本次的黑手就伸向了被人蹂躏无数次的小说《红楼梦》。<br /><br />折腾的思路基本上就是：1、将文本按章回划分。2、按章回获取某些统计量，看能否区别前八十回和后四十回。3、中文分词后以空间向量模型方法构造词条-文档关系矩阵。4、计算文档间距离后用多维标度方法降到二维展现120回的散点图形。<br /><br />先从网上扒拉下一个纯文本的文档，也不知道什么版本。先用记事本打开后转成UTF8格式，然后在R中读入资料。此时的存储格式是list格式，list的每一个元素是文本中的一段。首先找出文本的章节划分点，然后计算出每一章节的段落数。下图即是这120回的段落数条形图，似乎在后四十回段落数量较少，而且趋于稳定，不如前八十回变化大。对二者作一个wilcox检验，证实了数量较少的显著性。同时用置换检验也证实了这一点。<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-cVRCmkYnfLs/T8guUuDpjBI/AAAAAAAAA7w/xZWty1J5y08/s1600/Rplot.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-cVRCmkYnfLs/T8guUuDpjBI/AAAAAAAAA7w/xZWty1J5y08/s1600/Rplot.jpeg" /></a></div><br /><br /><a name='more'></a>除了段落，我们还可以观察不同章节的字数和句子数量。前者容易弄一些，后者是采用迂回的法子，去搜索文档中的几种标点，用标点数量来代表句子数量。先建立了一个搜索函数，然后分别搜索了句号、问号和感叹号的总数量。如下图所示，看起来没体现出什么模式。<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-cmYwxwF00mw/T8gu6WbPCKI/AAAAAAAAA74/aPud8XmLteY/s1600/Rplot01.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-cmYwxwF00mw/T8gu6WbPCKI/AAAAAAAAA74/aPud8XmLteY/s1600/Rplot01.jpeg" /></a></div><br />进一步的，可以将每个章节的句子数量除以段落数量，得到平均的段落所包含的句子数量&nbsp;。画成散点图，看起来后四十回的段落中句子数量较为一致，不象前八十回那么起伏变化。或许是写手为了仿照前文，文笔有些拘谨？<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-9l4tJxBv8Sk/T8gvIl-rTzI/AAAAAAAAA8A/ILRyhy22BLw/s1600/Rplot02.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-9l4tJxBv8Sk/T8gvIl-rTzI/AAAAAAAAA8A/ILRyhy22BLw/s1600/Rplot02.jpeg" /></a></div><br />还可以查询每个章节中几位主角名字的出现频次，在这里是用相对频率来表现三位主角名字在每章节的出现比例。黛玉啊，100回之后就没怎么出来了。<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-g2eKmjaqoSw/T8gv0zkh9MI/AAAAAAAAA8I/oyPezMQ9SAI/s1600/Rplot04.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-g2eKmjaqoSw/T8gv0zkh9MI/AAAAAAAAA8I/oyPezMQ9SAI/s1600/Rplot04.jpeg" /></a></div><br />下面我们再深入一步，用rmmseg4j包将文本进行分词，再用tm包构建语料库和词频矩阵。在最后生成词频矩阵时得到一个奇怪的结果，就是只获得了三个字以上词的矩阵，折腾许久也不知道是什么原因。就用这个结果，再用余弦距离来计算120回文本之间的疏离度，也就是距离。将这个距离矩阵用MDS降到二维来展现。从下图来看，前八十回和后四十回似乎分的比较清楚。呵呵，折腾完毕。<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-g7lvK1z3xlo/T8gwOI8jZOI/AAAAAAAAA8Q/XulCoXzIdg4/s1600/Rplot03.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-g7lvK1z3xlo/T8gwOI8jZOI/AAAAAAAAA8Q/XulCoXzIdg4/s1600/Rplot03.jpeg" /></a></div>代码<a href="https://gist.github.com/2848339">在此</a>，原文本<a href="https://www.box.com/s/ca33ebd08a7c84f190a8">在此</a>。欢迎大家来拍砖。<br /><br />
