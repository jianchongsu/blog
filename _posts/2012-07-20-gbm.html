--- 
layout: post
name: gbm
title: "\xE7\x94\xA8gbm\xE5\x8C\x85\xE5\xAE\x9E\xE7\x8E\xB0\xE9\x9A\x8F\xE6\x9C\xBA\xE6\xA2\xAF\xE5\xBA\xA6\xE6\x8F\x90\xE5\x8D\x87\xE7\xAE\x97\xE6\xB3\x95"
date: 2012-07-20 13:37:00 +08:00
categories: 
- "\xE5\x86\xB3\xE7\xAD\x96\xE6\xA0\x91"
- "\xE9\x9B\x86\xE6\x88\x90\xE5\xAD\xA6\xE4\xB9\xA0"
- "\xE6\x8F\x90\xE5\x8D\x87\xE7\xAE\x97\xE6\xB3\x95"
- "\xE6\x95\xB0\xE6\x8D\xAE\xE6\x8C\x96\xE6\x8E\x98"
permalink: /2012/07/gbm.html
---
<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-VR8JDGXCYJE/UAjqGzWwXWI/AAAAAAAABCg/E4ZrjzvhLmw/s1600/3man.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" height="140" src="http://4.bp.blogspot.com/-VR8JDGXCYJE/UAjqGzWwXWI/AAAAAAAABCg/E4ZrjzvhLmw/s200/3man.jpg" width="200" /></a></div><span style="background-color: white;">中国有句老话：三个臭皮匠，顶个诸葛亮。这个说法至少在变形金刚中得到了体现，没有组合之前的大力神只是五个可以被柱子哥随手秒掉工地苦力。但组合之后却是威力大增。在机器学习领域也是如此，一堆能力一般的“弱学习器”也能组合成一个“强学习器”。<a href="http://xccds1977.blogspot.com/2012/07/blog-post_15.html">前篇文章</a>提到的随机森林就是一种组合学习的方法，本文要说的是另一类组合金刚：<b>提升方法(Boosting)</b>。提升方法是一大类集成分类学习的统称。它用不同的权重将基学习器进行线性组合，使表现优秀的学习器得到重用。在<b>R语言中gbm包</b>就是用来实现一般提升方法的扩展包。根据基学习器、损失函数和优化方法的不同，提升方法也有各种不同的形式。</span><br /><br /><b>自适应提升方法AdaBoost</b><br />它是一种传统而重要的Boost算法，在学习时为每一个样本赋上一个权重，初始时各样本权重一样。在每一步训练后，增加错误学习样本的权重，这使得某些样本的重要性凸显出来，在进行了N次迭代后，将会得到N个简单的学习器。最后将它们组合起来得到一个最终的模型。<br /><br /><b>梯度提升方法Gradient Boosting</b><br />梯度提升算法初看起来不是很好理解，但我们和线性回归加以类比就容易了。回忆一下线性回归是希望找到一组参数使得残差最小化。如果只用一次项来解释二次曲线一定会有大量残差留下来，此时就可以用二次项来继续解释残差，所以可在模型中加入这个二次项。<br /><br />同样的，梯度提升是先根据初始模型计算伪残差，之后建立一个基学习器来解释伪残差，该基学习器是在梯度方向上减少残差。再将基学习器乘上权重系数(学习速率)和原来的模型进行线性组合形成新的模型。这样反复迭代就可以找到一个使损失函数的期望达到最小的模型。在训练基学习器时可以使用再抽样方法，此时就称之为<b>随机梯度提升算法stochastic gradient boosting</b>。<br /><br /><br /><a name='more'></a>在gbm包中，采用的是决策树作为基学习器，重要的参数设置如下：<br /><ul><li><span style="background-color: white;">损失函数的形式(distribution)</span></li><li><span style="background-color: white;">迭代次数(n.trees)</span></li><li><span style="background-color: white;">学习速率(shrinkage)</span></li><li><span style="background-color: white;">再抽样比率(bag.fraction)</span></li><li><span style="background-color: white;">决策树的深度(interaction.depth)</span></li></ul><span style="background-color: white;">损失函数的形式容易设定，分类问题一般选择bernoulli分布，而回归问题可以选择gaussian分布。学习速率方面，我们都知道步子迈得太大容易扯着，所以学习速率是越小越好，但是步子太小的话，步数就得增加，也就是训练的迭代次数需要加大才能使模型达到最优，这样训练所需时间和计算资源也相应加大了。gbm作者的经验法则是设置shrinkage参数在0.01-0.001之间，而n.trees参数在3000-10000之间。</span><br /><br />下面我们用mlbench包中的数据集来看一下gbm包的使用。其中响应变量为diabetes，即病人的糖尿病诊断是阳性还是阴性。<br /><div style="overflow: auto;"><div class="geshifilter"><pre class="r geshifilter-R"><span style="font-family: 'Courier New', Courier, monospace;"><span style="color: #666666; font-style: italic;"># 加载包和数据</span><br /><a href="http://inside-r.org/r-doc/base/library"><span style="color: #003399;">library</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/packages/cran/gbm">gbm</a><span style="color: #009900;">)</span><br /><a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a><span style="color: #009900;">(</span>PimaIndiansDiabetes2<span style="color: #339933;">,</span>package=<span style="color: blue;">'mlbench'</span><span style="color: #009900;">)</span><br /><span style="color: #666666; font-style: italic;"># 将响应变量转为0-1格式</span><br /><a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a> &lt;- PimaIndiansDiabetes2<br /><a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a>$diabetes &lt;- <a href="http://inside-r.org/r-doc/base/as.numeric"><span style="color: #003399;">as.numeric</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a>$diabetes<span style="color: #009900;">)</span><br /><a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a> &lt;- <a href="http://inside-r.org/r-doc/base/transform"><span style="color: #003399;">transform</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a><span style="color: #339933;">,</span>diabetes=diabetes-<span style="color: #cc66cc;">1</span><span style="color: #009900;">)</span><br /><span style="color: #666666; font-style: italic;"># 使用gbm函数建模</span><br />model &lt;- <a href="http://inside-r.org/packages/cran/gbm">gbm</a><span style="color: #009900;">(</span>diabetes~.<span style="color: #339933;">,</span><a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a>=<a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a><span style="color: #339933;">,</span>shrinkage=<span style="color: #cc66cc;">0.01</span><span style="color: #339933;">,</span><br />             distribution=<span style="color: blue;">'bernoulli'</span><span style="color: #339933;">,</span>cv.folds=<span style="color: #cc66cc;">5</span><span style="color: #339933;">,</span><br />             n.trees=<span style="color: #cc66cc;">3000</span><span style="color: #339933;">,</span>verbose=F<span style="color: #009900;">)</span><br /><span style="color: #666666; font-style: italic;"># 用交叉检验确定最佳迭代次数</span><br />best.iter &lt;- gbm.perf<span style="color: #009900;">(</span>model<span style="color: #339933;">,</span>method=<span style="color: blue;">'cv'</span><span style="color: #009900;">)</span></span></pre><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-oe0h9wHJRlk/UAjs1ikVuTI/AAAAAAAABCs/xLaQHolL09w/s1600/Rplot02.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-oe0h9wHJRlk/UAjs1ikVuTI/AAAAAAAABCs/xLaQHolL09w/s1600/Rplot02.jpeg" /></a></div><pre class="r geshifilter-R"><span style="font-family: 'Courier New', Courier, monospace;"><br /><span style="color: #666666; font-style: italic;"># 观察各解释变量的重要程度</span><br /><a href="http://inside-r.org/r-doc/base/summary"><span style="color: #003399;">summary</span></a><span style="color: #009900;">(</span>model<span style="color: #339933;">,</span>best.iter<span style="color: #009900;">)</span></span></pre><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-dkAKB-ErNLM/UAjtMj9gSzI/AAAAAAAABC0/YivOEPCbE5w/s1600/Rplot.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-dkAKB-ErNLM/UAjtMj9gSzI/AAAAAAAABC0/YivOEPCbE5w/s1600/Rplot.jpeg" /></a></div><pre class="r geshifilter-R"><span style="font-family: 'Courier New', Courier, monospace;"><br /><span style="color: #666666; font-style: italic;"># 变量的边际效应</span><br />plot.gbm<span style="color: #009900;">(</span>model<span style="color: #339933;">,</span><span style="color: #cc66cc;">1</span><span style="color: #339933;">,</span>best.iter<span style="color: #009900;">)</span></span></pre><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-oD93vZ1qPWE/UAjtRkCbP2I/AAAAAAAABC8/VBkPPFTIWpw/s1600/Rplot01.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-oD93vZ1qPWE/UAjtRkCbP2I/AAAAAAAABC8/VBkPPFTIWpw/s1600/Rplot01.jpeg" /></a></div><pre class="r geshifilter-R"><span style="font-family: 'Courier New', Courier, monospace;"><br /><span style="color: #666666; font-style: italic;"># 用caret包观察预测精度</span><br /><a href="http://inside-r.org/r-doc/base/library"><span style="color: #003399;">library</span></a><span style="color: #009900;">(</span><a href="http://inside-r.org/packages/cran/caret">caret</a><span style="color: #009900;">)</span><br /><a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a> &lt;- PimaIndiansDiabetes2<br />fitControl &lt;- trainControl<span style="color: #009900;">(</span>method = <span style="color: blue;">"cv"</span><span style="color: #339933;">,</span> number = <span style="color: #cc66cc;">5</span><span style="color: #339933;">,</span>returnResamp = <span style="color: blue;">"all"</span><span style="color: #009900;">)</span><br />model2 &lt;- train<span style="color: #009900;">(</span>diabetes~.<span style="color: #339933;">,</span> <a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a>=<a href="http://inside-r.org/r-doc/utils/data"><span style="color: #003399;">data</span></a><span style="color: #339933;">,</span>method=<span style="color: blue;">'gbm'</span><span style="color: #339933;">,</span>distribution=<span style="color: blue;">'bernoulli'</span><span style="color: #339933;">,</span>trControl = fitControl<span style="color: #339933;">,</span>verbose=F<span style="color: #339933;">,</span>tuneGrid = <a href="http://inside-r.org/r-doc/base/data.frame"><span style="color: #003399;">data.frame</span></a><span style="color: #009900;">(</span>.n.trees=best.iter<span style="color: #339933;">,</span>.shrinkage=<span style="color: #cc66cc;">0.01</span><span style="color: #339933;">,</span>.interaction.depth=<span style="color: #cc66cc;">1</span><span style="color: #009900;">)</span><span style="color: #009900;">)</span><br />model2</span></pre></div></div><br />&nbsp; Accuracy &nbsp;Kappa &nbsp;Accuracy SD &nbsp;Kappa SD<br />&nbsp; 0.78 &nbsp; &nbsp; &nbsp;0.504 &nbsp;0.0357 &nbsp; &nbsp; &nbsp; 0.0702 <br />观察到gbm迭代到800次左右最优，得到的预测正确率为0.78，这个比随机森林的正确率还要略高一些。<span style="background-color: white;">提升算法继承了单一决策树的优点，例如：能处理缺失数据，对于噪声数据不敏感，但又摒弃了它的缺点，使之能拟合复杂的非线性关系，精确度大为提高。通过控制迭代次数能控制过度拟合，计算速度快。但由于它是顺序计算的，所以不好进行分布式计算。</span><br /><div><span style="background-color: white;"><br /></span></div><br />参考资料：<br /><a href="http://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf">http://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf</a><br /><a href="http://en.wikipedia.org/wiki/Gradient_boosting">http://en.wikipedia.org/wiki/Gradient_boosting</a><br /><a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html">http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html</a>
