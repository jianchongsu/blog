--- 
layout: post
name: caret_24
title: "caret\xE5\x8C\x85\xE5\xBA\x94\xE7\x94\xA8\xE4\xB9\x8B\xE4\xBA\x8C\xEF\xBC\x9A\xE7\x89\xB9\xE5\xBE\x81\xE9\x80\x89\xE6\x8B\xA9"
date: 2011-09-25 09:53:00 +08:00
categories: 
- "\xE6\x95\xB0\xE6\x8D\xAE\xE6\x8C\x96\xE6\x8E\x98"
- "\xE7\x89\xB9\xE5\xBE\x81\xE9\x80\x89\xE6\x8B\xA9"
permalink: /2011/09/caret_24.html
---
在进行数据挖掘时，我们并不需要将所有的自变量用来建模，而是从中选择若干最重要的变量，这称为特征选择（feature selection）。一种算法就是后向选择，即先将所有的变量都包括在模型中，然后计算其效能（如误差、预测精度）和变量重要排序，然后保留最重要的若干变量，再次计算效能，这样反复迭代，找出合适的自变量数目。这种算法的一个缺点在于可能会存在过度拟合，所以需要在此算法外再套上一个样本划分的循环。在caret包中的rfe命令可以完成这项任务。<br /><a name='more'></a><br /><br />首先定义几个整数，程序必须测试这些数目的自变量.<br /><blockquote>subsets = c(20,30,40,50,60,70,80)</blockquote>然后定义控制参数，functions是确定用什么样的模型进行自变量排序，本例选择的模型是随机森林即rfFuncs，可以选择的还有lmFuncs（线性回归），nbFuncs（朴素贝叶斯），treebagFuncs（装袋决策树），caretFuncs（自定义的训练模型）。<br />method是确定用什么样的抽样方法，本例使用cv即交叉检验, 还有提升boot以及留一交叉检验LOOCV<br /><blockquote>ctrl= rfeControl(functions = rfFuncs, method = "cv",verbose = FALSE, returnResamp = "final")</blockquote>最后使用rfe命令进行特征选择，计算量很大，这得花点时间<br /><blockquote>Profile = rfe(newdata3, mdrrClass, sizes = subsets, rfeControl = ctrl)</blockquote>观察结果选择50个自变量时，其预测精度最高<br /><blockquote>print(Profile)</blockquote><blockquote><span class="Apple-style-span" style="font-family: 'Lucida Console'; font-size: 13px; line-height: 17px; white-space: pre-wrap;"> Variables Accuracy  Kappa AccuracySD KappaSD Selected</span></blockquote><blockquote><pre class="GD40030CKR" style="border-bottom-style: none; border-color: initial; border-left-style: none; border-right-style: none; border-top-style: none; border-width: initial; font-family: 'Lucida Console'; font-size: 10pt !important; line-height: 1.3; outline-color: initial; outline-style: none; outline-width: initial; white-space: pre-wrap !important;" tabindex="0">        20   0.8200 0.6285    0.04072 0.08550         <br />        30   0.8200 0.6294    0.04868 0.10102         <br />        40   0.8295 0.6487    0.03608 0.07359         <br />        50   0.8313 0.6526    0.04257 0.08744        *<br />        60   0.8277 0.6447    0.03477 0.07199         <br />        70   0.8276 0.6449    0.04074 0.08353         <br />        80   0.8275 0.6449    0.03991 0.08173         <br />        94   0.8313 0.6529    0.03899 0.08006</pre></blockquote>用图形也可以观察到同样结果<br /><blockquote>plot(Profile)</blockquote><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-Gg8LZs_v8Ac/Tn6JYf0jSJI/AAAAAAAAAb4/ygjyn1D5mkc/s1600/Rplot01.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="214" src="http://4.bp.blogspot.com/-Gg8LZs_v8Ac/Tn6JYf0jSJI/AAAAAAAAAb4/ygjyn1D5mkc/s320/Rplot01.jpeg" width="320" /></a></div><br />下面的命令则可以返回最终保留的自变量<br /><blockquote>Profile$optVariables</blockquote><br />
