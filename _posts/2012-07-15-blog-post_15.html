--- 
layout: post
name: blog-post_15
title: "\xE9\x9A\x8F\xE6\x9C\xBA\xE6\xA3\xAE\xE6\x9E\x97\xE5\x8F\x8A\xE5\x85\xB6\xE5\x89\xAF\xE4\xBA\xA7\xE5\x93\x81"
date: 2012-07-15 13:49:00 +08:00
categories: 
- "\xE5\x86\xB3\xE7\xAD\x96\xE6\xA0\x91"
- "\xE9\x9B\x86\xE6\x88\x90\xE5\xAD\xA6\xE4\xB9\xA0"
- "\xE9\x9A\x8F\xE6\x9C\xBA\xE6\xA3\xAE\xE6\x9E\x97"
- "\xE6\x95\xB0\xE6\x8D\xAE\xE6\x8C\x96\xE6\x8E\x98"
- "\xE7\x89\xB9\xE5\xBE\x81\xE9\x80\x89\xE6\x8B\xA9"
permalink: /2012/07/blog-post_15.html
---
<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-4gs8lXw45p4/UAJYEEyn7xI/AAAAAAAABCU/05kJSa_zX9s/s1600/1.jpg" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-4gs8lXw45p4/UAJYEEyn7xI/AAAAAAAABCU/05kJSa_zX9s/s1600/1.jpg" /></a></div><span style="background-color: white;"><b>随机森林(Random Forest)</b>方法是Leo Breiman于2001年提出的一种<b>集成学习（Ensemble Learning）</b>方法，它是传统决策树方法的扩展，将多个决策树进行组合，来提高预测精度。随机森林利用分类回归树(CART)作为其基本组成单元，也可称之为基学习器或是子模型。CART在<a href="http://xccds1977.blogspot.com/2011/06/cartr.html">之前的文章</a>中我们已经介绍过，就不再详细说明了。而集成学习的思路是试图通过连续调用单个学习算法，获得不同的学习器，然后根据规则组合这些学习器来解决同一个问题，可以显著的提高学习系统的泛化能力。组合多个学习器主要采用加权平均或投票的方法。常见的集成学习算法还包括了装袋算法（Bagging)和提升算法（Boosting）。</span><br /><br /><b>1. 随机森林计算步骤</b><br /><br /><ul><li><span style="background-color: white;">从原始训练样本中随机有放回抽出N个样本；</span></li><li><span style="background-color: white;">从解释变量中随机抽出M个变量；</span></li><li><span style="background-color: white;">依据上述得到的子集实施CART方法（无需剪枝），从而形成一个单独的决策树；</span></li><li><span style="background-color: white;">重复上面步骤X次，就构建了有X棵树的随机森林模型。</span></li><li><span style="background-color: white;">在对新数据进行预测分类时，由X棵树分别预测，以投票方式综合最终结果。</span></li></ul><span style="background-color: white;"><b></b></span><br /><a name='more'></a><span style="background-color: white;"><b>2. 随机森林的特点</b></span><br /><br /><ul><li><span style="background-color: white;">相对其它算法，准确率很高；</span></li><li><span style="background-color: white;">不会形成过度拟合；</span></li><li><span style="background-color: white;">速度快，能够处理大数据；</span></li><li><span style="background-color: white;">能处理很高维度的数据，不用做特征选择；</span></li><li><span style="background-color: white;">由于抽样的原因，会有一些未被抽中的样本，这形成了所谓的袋外数据(OOB)，可以根据OOB来估计泛化误差，而不需要用交叉检验来估计。</span></li></ul><br /><span style="background-color: white;"><b>3. 随机森林的副产品</b></span><br /><span style="background-color: white;"><b><br /></b></span><br />除了能用于回归分类之外，它还可以提供一些其它很有价值的功能。R语言中的randomForest包中就包括了这些函数。<br /><br /><b>副产品之一，判断变量的重要程度。</b><span style="background-color: white;">由于决策树是根据不同变量来分割数据，所以一棵树中能进行正确划分的变量就是最重要的变量。随机森林可以根据置换划分变量对分类误差的影响，来判断哪些变量是比较重要的。这个功能非常实用，特别在处理变量极多的数据集，可以用它来作为变量选择的过滤器，然后再使用其它分类方法。randomForest包中的importance函数能返回各变量的重要程度，varImpplot函数可以用图形方式加以展现。partialPlot函数则能呈现变量的偏效应。rfcv函数用来得到最优的的变量数目。</span><br /><span style="background-color: white;"><br /></span><br /><b>副产品之二，度量样本间的相似程度。</b><span style="background-color: white;">决策树的理念是将数据归入不同的组中，那么同一组中的样本可以认为是比较相似的。根据这个思路可以建立起各样本间的相似矩阵。用1-相似矩阵则可以认为是一种“距离”，利用距离就可以进行异常值检验或聚类分析。outlier函数可以返回各样本的离群值，值越大表示越有可能是异常点。cluster包中的pam函数和kmeans函数相近，但它可以接受距离矩阵作为参数。</span><br /><span style="background-color: white;"><br /></span><br /><b>副产品之三，缺失数据的插补。</b><span style="background-color: white;">处理缺失数据有一种初级方法，即对数值变量，用中位数来代替，对于分类变量，用频数最高的类来代替缺失值。rfImpute函数首先使用初级方法来插补，然后计算近似矩阵，再用近似度为权重再次加权计算缺失值。然后再次计算近似矩阵，这样反复迭代。rfImpute函数能实现这一功能。</span><br /><br />最后还值得提到的是处理不平衡数据，可以在主函数randomForest中加入classwt参数设置，将数据较多的类设置为较大的权数，这样可以在一定程度上修正数据不平衡的影响，使占少数类别的预测准确率提高。当然这样做的代价是总误差水平上升了。<br /><br />参考资料：<br /><a href="http://www.webchem.science.ru.nl/PRiNS/rF.pdf">http://www.webchem.science.ru.nl/PRiNS/rF.pdf</a><br /><a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a>
