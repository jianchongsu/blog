--- 
layout: post
name: glmnetlasso
title: "\xE7\x94\xA8glmnet\xE5\x8C\x85\xE5\xAE\x9E\xE6\x96\xBD\xE5\xA5\x97\xE7\xB4\xA2\xE7\xAE\x97\xE6\xB3\x95(LASSO)"
date: 2012-05-19 11:35:00 +08:00
categories: 
- ggplot2
- "logistic\xE5\x9B\x9E\xE5\xBD\x92"
- LASSO
permalink: /2012/05/glmnetlasso.html
---
<br /><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-izhFxLWrXXg/T7cUk7vkxMI/AAAAAAAAA7E/HdmyfRdyy2c/s1600/3.JPG" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" height="143" src="http://3.bp.blogspot.com/-izhFxLWrXXg/T7cUk7vkxMI/AAAAAAAAA7E/HdmyfRdyy2c/s200/3.JPG" width="200" /></a></div>当我们使用数据训练分类器的时候，很重要的一点就是要在过度拟合与拟合不足之间达成一个平衡。防止过度拟合的一种方法就是对模型的复杂度进行约束。模型中用到解释变量的个数是模型复杂度的一种体现。控制解释变量个数有很多方法，例如<b>变量选择(feature selection)</b>，即用filter或wrapper方法提取解释变量的最佳子集。或是进行<b>变量构造(feature construction)</b>，即将原始变量进行某种映射或转换，如主成分方法和因子分析。变量选择的方法是比较“硬”的方法，变量要么进入模型，要么不进入模型，只有0-1两种选择。但也有“软”的方法，也就是<b>Regularization</b>类方法，例如<b>岭回归(Ridge Regression)</b>和<b>套索方法(LASSO:least absolute shrinkage and selection operator)</b>。<br /><br />这两种方法的共同点在于，将解释变量的系数加入到Cost Function中，并对其进行最小化，本质上是对过多的参数实施了惩罚。而两种方法的区别在于惩罚函数不同。但这种微小的区别却使LASSO有很多优良的特质（可以同时选择和缩减参数）。下面的公式就是在线性模型中两种方法所对应的目标函数：<br /><br /><a name='more'></a><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-k3qpu8X_L6s/T7cQ92IozlI/AAAAAAAAA6c/Oxo84yo2EZY/s1600/1.JPG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><br /><img border="0" height="64" src="http://1.bp.blogspot.com/-k3qpu8X_L6s/T7cQ92IozlI/AAAAAAAAA6c/Oxo84yo2EZY/s400/1.JPG" width="400" /></a></div><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-mOJIF1Z_Phc/T7cRFyOccvI/AAAAAAAAA6k/A7Srgc4Nozc/s1600/2.JPG" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="67" src="http://1.bp.blogspot.com/-mOJIF1Z_Phc/T7cRFyOccvI/AAAAAAAAA6k/A7Srgc4Nozc/s400/2.JPG" width="400" /></a></div>公式中的lambda是重要的设置参数，它控制了惩罚的严厉程度，如果设置得过大，那么最后的模型参数均将趋于0，形成拟合不足。如果设置得过小，又会形成拟合过度。所以lambda的取值一般需要通过交叉检验来确定。<br /><br />在R语言中可以使用<b>glmnet包</b>来实施套索算法。我们采用的数据集是Machine Learning公开课中第七课的<a href="https://class.coursera.org/ml/assignment/view?assignment_id=3">一个算例</a>。先来看看这个样本数据的散点图。下图显示有两个类别等待我们来区分。显然其决策边界是非线性的，所以如果要用<b>Logistic Regression</b>来作分类器的话，解释变量需要是多项式形式。但这里存在一个问题，我们应该用几阶的多项式呢？如果阶数过高，模型变量过多，会存在过度拟合，而反之阶数过少，又会存在拟合不足。所以这里我们用LASSO方法来建立Logistic回归分类器。<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-VqFOJ3oMDwM/T7cRd8UUg-I/AAAAAAAAA6s/a35WroHHuxs/s1600/Rplot.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-VqFOJ3oMDwM/T7cRd8UUg-I/AAAAAAAAA6s/a35WroHHuxs/s1600/Rplot.jpeg" /></a></div>分析步骤如下：<br /><br /><ul><li>根据算例要求，先生成有六阶多项式的自变量，这样一共有28个自变量；</li><li>用glmnet包中的cv.glmnet函数建模，该函数自带交叉检验功能；</li><li>根据上面的结果绘制CV图如下，从中选择最佳lambda值。</li></ul><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-IvHo0oQfOsE/T7cSIPeBy6I/AAAAAAAAA60/MbITszB0syY/s1600/Rplot01.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-IvHo0oQfOsE/T7cSIPeBy6I/AAAAAAAAA60/MbITszB0syY/s1600/Rplot01.jpeg" /></a></div><b>cv.glmnet</b>函数利用交叉检验，分别用不同的lambda值来观察模型误差。上图横轴是lambda值的对数，纵轴是模型误差。从上面的图可以看到，最佳的lambda取值就是在红色曲线的最低点处，对应着变量个数是11个。它右侧的另一条虚线是在其一倍SE内的更简洁的模型（变量个数为9）。由于这两个lambda对应的模型误差变化不大，而我们更偏好于简洁的模型，选择对应的lambda值为0.025。<br /><br />在使用cv.glmnet函数选择lambda值之后，我们没有必要去运行glmnet函数，直接从结果中就可以提取最终模型（9个变量）并获得参数系数。为了利于比较我们还提取了原始模型（28个变量）的参数系数。<br /><br />最后我们要在原来的散点图上画出两条决策边界，一条是根据LASSO方法得到的9变量模型，下图中紫色曲线即是它决策边界，决策边界比较平滑，具备很好的泛化能力。另一条是28个变量的原始模型。&nbsp;蓝色曲线即是它的决策边界，它为了拟合个别样本，显得凸凹不平。<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-v_d6iR1eQQ0/T7cSoWnDZaI/AAAAAAAAA68/wUb69KNrHlU/s1600/Rplot02.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-v_d6iR1eQQ0/T7cSoWnDZaI/AAAAAAAAA68/wUb69KNrHlU/s1600/Rplot02.jpeg" /></a></div>如果你的数据变异较大，那么在做LASSO之前最好进行数据标准化处理。LASSO的进一步扩展是和岭回归相结合，形成Elastic Net方法。glmnet包也可以实施这种算法。<br /><br /><b>参考资料：</b><br />《The Elements of Statistical Learning》<br />《Machine Learning for Hackers》<br /><a href="http://www-stat.stanford.edu/~tibs/lasso.html">http://www-stat.stanford.edu/~tibs/lasso.html</a><br /><a href="http://datamining.dongguk.ac.kr/ftp/temp/Regularization.pdf">http://datamining.dongguk.ac.kr/ftp/temp/Regularization.pdf</a><br /><a href="http://ygc.name/2011/10/26/machine-learning-5-2-regularized-logistic-regression/">http://ygc.name/2011/10/26/machine-learning-5-2-regularized-logistic-regression/</a><br /><br />本例的R代码<a href="https://gist.github.com/2728866">在此</a>。
